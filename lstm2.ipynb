{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ad757a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import f1_score\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "class SquatRepDataset(Dataset):\n",
    "    def __init__(self, data_list, label_list):\n",
    "        self.data_list = data_list  # list of dicts like the one you provided\n",
    "        self.label_list = label_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data_list[idx]\n",
    "        sequence = torch.tensor([\n",
    "            data[\"knee_angle\"],\n",
    "            data[\"torso_angle\"],\n",
    "            data[\"hip_angle\"],\n",
    "            data[\"symmetry_score\"],\n",
    "            data[\"alignment_score\"],\n",
    "            data[\"head_angle\"],\n",
    "            data[\"inter_thigh_angle\"],\n",
    "            data[\"heel_angle\"],\n",
    "            data[\"back_angle\"]\n",
    "        ], dtype=torch.float).T  # shape: [seq_len, num_features]\n",
    "\n",
    "        label = torch.tensor(self.label_list[idx], dtype=torch.long)\n",
    "        return sequence, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb277d73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PureConvClassifier(nn.Module):\n",
    "    def __init__(self, in_ch=9, conv_ch=[64,128,256], kernel=3, num_classes=6):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev = in_ch\n",
    "        for ch in conv_ch:\n",
    "            layers += [\n",
    "                nn.Conv1d(prev, ch, kernel, padding=kernel//2),\n",
    "                nn.BatchNorm1d(ch),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(2)\n",
    "            ]\n",
    "            prev = ch\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.global_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc1 = nn.Linear(prev, prev//2)\n",
    "        self.fc2 = nn.Linear(prev//2, num_classes)\n",
    "        \n",
    "    def forward(self, x_packed):\n",
    "        x, lengths = nn.utils.rnn.pad_packed_sequence(x_packed, batch_first=True)\n",
    "        x = x.transpose(1,2)              \n",
    "        x = self.net(x)                   \n",
    "        x = self.global_pool(x).squeeze(2) \n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d929f515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data prep\n",
    "\n",
    "\n",
    "def squat_collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    lengths = torch.tensor([seq.shape[0] for seq in sequences])  \n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True)\n",
    "    labels = torch.stack(labels)\n",
    "    return padded_sequences, lengths, labels\n",
    "\n",
    "\n",
    "\n",
    "bad_data_list=glob('temp_data/*')\n",
    "future = (bad_data_list)\n",
    "bad_list_new_list=[]\n",
    "labels_list=[]\n",
    "for i in range(len(bad_data_list)):\n",
    "    bad_list_new_list.extend(glob(bad_data_list[i]+'/*.json'))\n",
    "    one_hot=[0]*6\n",
    "    one_hot[i]=1\n",
    "    # print(one_hot)\n",
    "    for i in range(len(glob(bad_data_list[i]+'/*.json'))):\n",
    "        labels_list.append(one_hot)\n",
    "    # break\n",
    "# print(labels_list[:2])\n",
    "\n",
    "bad_data_list_new=[json.load(open(file)) for file in bad_list_new_list]\n",
    "# print(bad_list_new_list[0])\n",
    "with open(bad_list_new_list[0]) as f:\n",
    "    data = json.load(f)\n",
    "# print(data[1])\n",
    "bad_data_list_new_actual=[]\n",
    "label_list_actual=[]\n",
    "for i in bad_data_list_new:\n",
    "    for k in range(len(i)):\n",
    "        label_list_actual.append(labels_list[bad_data_list_new.index(i)])\n",
    "    for j in i:\n",
    "        bad_data_list_new_actual.append(j)\n",
    "bad_data_list=bad_data_list_new_actual\n",
    "bad_label_list=label_list_actual\n",
    "# print(data_list[0])\n",
    "# print(bad_data_list[0])\n",
    "# print(bad_label_list[0])\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "# data_list = [your_json_dict]  # can add more samples here\n",
    "# bad_label_list = [0]*len(bad_data_list_new)  # class index for this sample\n",
    "\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "dataset = SquatRepDataset(bad_data_list, bad_label_list)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size   = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    collate_fn=squat_collate_fn\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=squat_collate_fn\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 | Train Loss: 0.4379 | Val Loss: 0.3972 | Val F1: 0.194\n",
      "  ↳ New best model saved (val_loss = 0.3972)\n",
      "\n",
      "Epoch   2 | Train Loss: 0.4080 | Val Loss: 0.3690 | Val F1: 0.238\n",
      "  ↳ New best model saved (val_loss = 0.3690)\n",
      "\n",
      "Epoch   3 | Train Loss: 0.3633 | Val Loss: 0.3901 | Val F1: 0.347\n",
      "Epoch   4 | Train Loss: 0.3441 | Val Loss: 0.3038 | Val F1: 0.420\n",
      "  ↳ New best model saved (val_loss = 0.3038)\n",
      "\n",
      "Epoch   5 | Train Loss: 0.3046 | Val Loss: 0.3277 | Val F1: 0.370\n",
      "Epoch   6 | Train Loss: 0.2958 | Val Loss: 0.3386 | Val F1: 0.461\n",
      "Epoch   7 | Train Loss: 0.2703 | Val Loss: 0.2643 | Val F1: 0.605\n",
      "  ↳ New best model saved (val_loss = 0.2643)\n",
      "\n",
      "Epoch   8 | Train Loss: 0.2197 | Val Loss: 0.2571 | Val F1: 0.615\n",
      "  ↳ New best model saved (val_loss = 0.2571)\n",
      "\n",
      "Epoch   9 | Train Loss: 0.2120 | Val Loss: 0.2910 | Val F1: 0.605\n",
      "Epoch  10 | Train Loss: 0.2140 | Val Loss: 0.2550 | Val F1: 0.648\n",
      "  ↳ New best model saved (val_loss = 0.2550)\n",
      "\n",
      "Epoch  11 | Train Loss: 0.1492 | Val Loss: 0.2275 | Val F1: 0.716\n",
      "  ↳ New best model saved (val_loss = 0.2275)\n",
      "\n",
      "Epoch  12 | Train Loss: 0.1513 | Val Loss: 0.2466 | Val F1: 0.662\n",
      "Epoch  13 | Train Loss: 0.1310 | Val Loss: 0.2380 | Val F1: 0.682\n",
      "Epoch  14 | Train Loss: 0.1299 | Val Loss: 0.2403 | Val F1: 0.714\n",
      "Epoch  15 | Train Loss: 0.1166 | Val Loss: 0.2084 | Val F1: 0.702\n",
      "  ↳ New best model saved (val_loss = 0.2084)\n",
      "\n",
      "Epoch  16 | Train Loss: 0.0781 | Val Loss: 0.2583 | Val F1: 0.694\n",
      "Epoch  17 | Train Loss: 0.0718 | Val Loss: 0.2641 | Val F1: 0.712\n",
      "Epoch  18 | Train Loss: 0.0750 | Val Loss: 0.2182 | Val F1: 0.723\n",
      "Epoch  19 | Train Loss: 0.0370 | Val Loss: 0.1985 | Val F1: 0.747\n",
      "  ↳ New best model saved (val_loss = 0.1985)\n",
      "\n",
      "Epoch  20 | Train Loss: 0.0461 | Val Loss: 0.2240 | Val F1: 0.749\n",
      "Epoch  21 | Train Loss: 0.0563 | Val Loss: 0.2541 | Val F1: 0.717\n",
      "Epoch  22 | Train Loss: 0.0734 | Val Loss: 0.2440 | Val F1: 0.762\n",
      "Epoch  23 | Train Loss: 0.0448 | Val Loss: 0.2152 | Val F1: 0.754\n",
      "Epoch  24 | Train Loss: 0.0468 | Val Loss: 0.2198 | Val F1: 0.719\n",
      "Epoch  25 | Train Loss: 0.0220 | Val Loss: 0.3044 | Val F1: 0.679\n",
      "Epoch 00025: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch  26 | Train Loss: 0.0142 | Val Loss: 0.2119 | Val F1: 0.774\n",
      "Epoch  27 | Train Loss: 0.0091 | Val Loss: 0.2285 | Val F1: 0.766\n",
      "Epoch  28 | Train Loss: 0.0089 | Val Loss: 0.2194 | Val F1: 0.771\n",
      "Epoch  29 | Train Loss: 0.0083 | Val Loss: 0.2060 | Val F1: 0.779\n",
      "Epoch  30 | Train Loss: 0.0080 | Val Loss: 0.2355 | Val F1: 0.763\n",
      "Epoch  31 | Train Loss: 0.0069 | Val Loss: 0.2074 | Val F1: 0.778\n",
      "Epoch 00031: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch  32 | Train Loss: 0.0065 | Val Loss: 0.2108 | Val F1: 0.792\n",
      "Epoch  33 | Train Loss: 0.0093 | Val Loss: 0.2118 | Val F1: 0.795\n",
      "Epoch  34 | Train Loss: 0.0058 | Val Loss: 0.2204 | Val F1: 0.795\n",
      "Epoch  35 | Train Loss: 0.0041 | Val Loss: 0.2265 | Val F1: 0.777\n",
      "Epoch  36 | Train Loss: 0.0047 | Val Loss: 0.2264 | Val F1: 0.791\n",
      "Epoch  37 | Train Loss: 0.0036 | Val Loss: 0.2380 | Val F1: 0.764\n",
      "Epoch 00037: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch  38 | Train Loss: 0.0029 | Val Loss: 0.2267 | Val F1: 0.791\n",
      "Epoch  39 | Train Loss: 0.0039 | Val Loss: 0.2328 | Val F1: 0.801\n",
      "Epoch  40 | Train Loss: 0.0065 | Val Loss: 0.2302 | Val F1: 0.792\n",
      "Epoch  41 | Train Loss: 0.0057 | Val Loss: 0.2373 | Val F1: 0.769\n",
      "Epoch  42 | Train Loss: 0.0042 | Val Loss: 0.2334 | Val F1: 0.776\n",
      "Epoch  43 | Train Loss: 0.0036 | Val Loss: 0.2349 | Val F1: 0.778\n",
      "Epoch 00043: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch  44 | Train Loss: 0.0029 | Val Loss: 0.2303 | Val F1: 0.799\n",
      "Epoch  45 | Train Loss: 0.0030 | Val Loss: 0.2303 | Val F1: 0.785\n",
      "Epoch  46 | Train Loss: 0.0036 | Val Loss: 0.2282 | Val F1: 0.789\n",
      "Epoch  47 | Train Loss: 0.0052 | Val Loss: 0.2294 | Val F1: 0.777\n",
      "Epoch  48 | Train Loss: 0.0028 | Val Loss: 0.2277 | Val F1: 0.791\n",
      "Epoch  49 | Train Loss: 0.0024 | Val Loss: 0.2276 | Val F1: 0.800\n",
      "Epoch 00049: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch  50 | Train Loss: 0.0076 | Val Loss: 0.2351 | Val F1: 0.790\n",
      "Epoch  51 | Train Loss: 0.0021 | Val Loss: 0.2262 | Val F1: 0.782\n",
      "Epoch  52 | Train Loss: 0.0032 | Val Loss: 0.2222 | Val F1: 0.802\n",
      "Epoch  53 | Train Loss: 0.0019 | Val Loss: 0.2229 | Val F1: 0.798\n",
      "Epoch  54 | Train Loss: 0.0034 | Val Loss: 0.2222 | Val F1: 0.804\n",
      "Epoch  55 | Train Loss: 0.0024 | Val Loss: 0.2200 | Val F1: 0.791\n",
      "Epoch 00055: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Epoch  56 | Train Loss: 0.0037 | Val Loss: 0.2194 | Val F1: 0.798\n",
      "Epoch  57 | Train Loss: 0.0035 | Val Loss: 0.2184 | Val F1: 0.793\n",
      "Epoch  58 | Train Loss: 0.0024 | Val Loss: 0.2243 | Val F1: 0.794\n",
      "Epoch  59 | Train Loss: 0.0016 | Val Loss: 0.2236 | Val F1: 0.799\n",
      "Epoch  60 | Train Loss: 0.0021 | Val Loss: 0.2223 | Val F1: 0.784\n",
      "Epoch  61 | Train Loss: 0.0024 | Val Loss: 0.2241 | Val F1: 0.795\n",
      "Epoch 00061: reducing learning rate of group 0 to 7.8125e-06.\n",
      "Epoch  62 | Train Loss: 0.0029 | Val Loss: 0.2231 | Val F1: 0.800\n",
      "Epoch  63 | Train Loss: 0.0017 | Val Loss: 0.2201 | Val F1: 0.801\n",
      "Epoch  64 | Train Loss: 0.0036 | Val Loss: 0.2196 | Val F1: 0.801\n",
      "Epoch  65 | Train Loss: 0.0021 | Val Loss: 0.2230 | Val F1: 0.810\n",
      "Epoch  66 | Train Loss: 0.0017 | Val Loss: 0.2251 | Val F1: 0.780\n",
      "Epoch  67 | Train Loss: 0.0041 | Val Loss: 0.2223 | Val F1: 0.791\n",
      "Epoch 00067: reducing learning rate of group 0 to 3.9063e-06.\n",
      "Epoch  68 | Train Loss: 0.0023 | Val Loss: 0.2221 | Val F1: 0.797\n",
      "Epoch  69 | Train Loss: 0.0023 | Val Loss: 0.2243 | Val F1: 0.804\n",
      "Epoch  70 | Train Loss: 0.0043 | Val Loss: 0.2239 | Val F1: 0.805\n",
      "Epoch  71 | Train Loss: 0.0018 | Val Loss: 0.2278 | Val F1: 0.786\n",
      "Epoch  72 | Train Loss: 0.0019 | Val Loss: 0.2285 | Val F1: 0.806\n",
      "Epoch  73 | Train Loss: 0.0020 | Val Loss: 0.2236 | Val F1: 0.803\n",
      "Epoch 00073: reducing learning rate of group 0 to 1.9531e-06.\n",
      "Epoch  74 | Train Loss: 0.0029 | Val Loss: 0.2177 | Val F1: 0.791\n",
      "Epoch  75 | Train Loss: 0.0017 | Val Loss: 0.2249 | Val F1: 0.785\n",
      "Epoch  76 | Train Loss: 0.0026 | Val Loss: 0.2352 | Val F1: 0.804\n",
      "Epoch  77 | Train Loss: 0.0032 | Val Loss: 0.2177 | Val F1: 0.797\n",
      "Epoch  78 | Train Loss: 0.0032 | Val Loss: 0.2245 | Val F1: 0.797\n",
      "Epoch  79 | Train Loss: 0.0104 | Val Loss: 0.2218 | Val F1: 0.800\n",
      "Epoch 00079: reducing learning rate of group 0 to 9.7656e-07.\n",
      "Epoch  80 | Train Loss: 0.0079 | Val Loss: 0.2362 | Val F1: 0.800\n",
      "Epoch  81 | Train Loss: 0.0024 | Val Loss: 0.2326 | Val F1: 0.789\n",
      "Epoch  82 | Train Loss: 0.0019 | Val Loss: 0.2212 | Val F1: 0.801\n",
      "Epoch  83 | Train Loss: 0.0030 | Val Loss: 0.2272 | Val F1: 0.799\n",
      "Epoch  84 | Train Loss: 0.0041 | Val Loss: 0.2221 | Val F1: 0.790\n",
      "Epoch  85 | Train Loss: 0.0021 | Val Loss: 0.2261 | Val F1: 0.803\n",
      "Epoch 00085: reducing learning rate of group 0 to 4.8828e-07.\n",
      "Epoch  86 | Train Loss: 0.0031 | Val Loss: 0.2241 | Val F1: 0.793\n",
      "Epoch  87 | Train Loss: 0.0027 | Val Loss: 0.2261 | Val F1: 0.799\n",
      "Epoch  88 | Train Loss: 0.0040 | Val Loss: 0.2285 | Val F1: 0.793\n",
      "Epoch  89 | Train Loss: 0.0026 | Val Loss: 0.2205 | Val F1: 0.798\n",
      "Epoch  90 | Train Loss: 0.0017 | Val Loss: 0.2233 | Val F1: 0.805\n",
      "Epoch  91 | Train Loss: 0.0023 | Val Loss: 0.2195 | Val F1: 0.793\n",
      "Epoch 00091: reducing learning rate of group 0 to 2.4414e-07.\n",
      "Epoch  92 | Train Loss: 0.0017 | Val Loss: 0.2190 | Val F1: 0.792\n",
      "Epoch  93 | Train Loss: 0.0018 | Val Loss: 0.2251 | Val F1: 0.791\n",
      "Epoch  94 | Train Loss: 0.0014 | Val Loss: 0.2217 | Val F1: 0.791\n",
      "Epoch  95 | Train Loss: 0.0026 | Val Loss: 0.2300 | Val F1: 0.807\n",
      "Epoch  96 | Train Loss: 0.0021 | Val Loss: 0.2236 | Val F1: 0.788\n",
      "Epoch  97 | Train Loss: 0.0021 | Val Loss: 0.2265 | Val F1: 0.791\n",
      "Epoch 00097: reducing learning rate of group 0 to 1.2207e-07.\n",
      "Epoch  98 | Train Loss: 0.0020 | Val Loss: 0.2285 | Val F1: 0.799\n",
      "Epoch  99 | Train Loss: 0.0020 | Val Loss: 0.2277 | Val F1: 0.799\n",
      "Epoch 100 | Train Loss: 0.0016 | Val Loss: 0.2255 | Val F1: 0.801\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = PureConvClassifier().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for X_batch, lengths, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        packed  = nn.utils.rnn.pack_padded_sequence(\n",
    "                      X_batch, lengths, batch_first=True, enforce_sorted=False\n",
    "                  )\n",
    "        logits  = model(packed)\n",
    "        loss    = criterion(logits, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, lengths, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device).float()\n",
    "\n",
    "            packed = nn.utils.rnn.pack_padded_sequence(\n",
    "                         X_batch, lengths,\n",
    "                         batch_first=True,\n",
    "                         enforce_sorted=False\n",
    "                     )\n",
    "            logits = model(packed)\n",
    "            loss   = criterion(logits, y_batch)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs >= 0.5).long()\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_targets.append(y_batch.cpu().long())\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    all_preds   = torch.cat(all_preds, dim=0).numpy()\n",
    "    all_targets = torch.cat(all_targets, dim=0).numpy()\n",
    "    val_f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:3d} | \"\n",
    "        f\"Train Loss: {train_loss:.4f} | \"\n",
    "        f\"Val Loss: {val_loss:.4f} | \"\n",
    "        f\"Val F1: {val_f1:.3f}\"\n",
    "    )\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(f\"  ↳ New best model saved (val_loss = {val_loss:.4f})\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pedofile = json.load(open(\"rep_metrics.json\"))\n",
    "actual_list = []\n",
    "for j in pedofile:\n",
    "    actual_list.append(j)\n",
    "pedo_input = [torch.tensor([\n",
    "            data[\"knee_angle\"],\n",
    "            data[\"torso_angle\"],\n",
    "            data[\"hip_angle\"],\n",
    "            data[\"symmetry_score\"],\n",
    "            data[\"alignment_score\"],\n",
    "            data[\"head_angle\"],\n",
    "            data[\"inter_thigh_angle\"],\n",
    "            data[\"heel_angle\"],\n",
    "            data[\"back_angle\"]\n",
    "        ], dtype=torch.float).T  \n",
    "        for data in actual_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9b617689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4, device='cuda:0')\n",
      "preds: tensor([0, 0, 0, 0, 1, 0], device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "preds: tensor([0, 0, 0, 0, 1, 0], device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "preds: tensor([0, 0, 0, 0, 1, 0], device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n",
      "preds: tensor([0, 0, 0, 0, 1, 0], device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "preds: tensor([0, 0, 0, 1, 0, 0], device='cuda:0')\n",
      "['temp_data/bad_inner_thigh', 'temp_data/bad_shallow', 'temp_data/good', 'temp_data/bad_head', 'temp_data/bad_back_warp', 'temp_data/bad_toe']\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for seq in pedo_input:\n",
    "        seq = seq.to(device)\n",
    "\n",
    "        seq = seq.unsqueeze(0)\n",
    "\n",
    "        lengths = [seq.size(1)]    \n",
    "\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            seq,\n",
    "            lengths,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        logits = model(packed)     \n",
    "        probs  = torch.sigmoid(logits)\n",
    "        preds  = (probs >= 0.5).long()\n",
    "        print(torch.argmax(probs))\n",
    "        print(f\"preds: {preds.squeeze(0)}\")\n",
    "print(future)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
